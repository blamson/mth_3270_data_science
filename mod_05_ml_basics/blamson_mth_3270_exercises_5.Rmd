---
title: "Data Science Module 5 Exercises"
author: "Brady Lamson"
date: "2/28/2022"
output: 
    pdf_document:
        dev: png
        includes:
            in_header: "preamble.tex"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, out.width = "60%")
library(dplyr)
library(ggplot2)
library(kableExtra)
```

# 9: Statistical Foundations
## 9.3: Simulations
### Exercise 1:

```{r}
# A
sample_mean_vec <- c()

for(i in 1:1000) {
    sim_sample <- rnorm(n = 10, mean = 50, sd = 15)
    sample_mean_vec <- c(sample_mean_vec, mean(sim_sample))
}
```

```{r, echo=TRUE}
#B
sample_mean <- mean(sample_mean_vec) %>% round(digits = 3)
sample_standard_error <- sd(sample_mean_vec) %>% round(digits = 3)

glue::glue("
    The mean of the sample mean vector is approximately {sample_mean},
    and the standard error of the vector is approximately {sample_standard_error}.
")
```
c) We can see that both values we got are very close to their theoretical values. The theoretical mean would be 50 and we can calculate the theoretical standard error, $\sigma / \sqrt{n} = 15/\sqrt{10} = 4.74342$. 

```{r}
# D

ggplot(data = data.frame(sample_mean_vec)) +
    geom_histogram(
        mapping = aes(x = sample_mean_vec, y = stat(density)), 
        binwidth = 1, 
        color = "black") +
    geom_function(
        fun = dnorm, 
        args = list(mean = 50, sd = 15/sqrt(10)), 
        color = "blue") +
    labs(title = "Sampling Distribution")
```

The blue line represents an idealized normal distribution. What we can see is that our simulation comes incredibly close. The center is right around 50, the shape follows the same curve and the density matches as well. 

***
\pagebreak
### Exercise 2

```{r}
sample_mean_vec <- c()
sample_median_vec <- c()
sample_sd_vec <- c()
sample_min_vec <- c()
sample_max_vec <- c()
sim_vec <- c()

for(i in 1:1000) {
    sim_sample <- rnorm(n = 5, mean = 50, sd = 15)
    sim_vec <- c(sim_vec, sim_sample)
    # simulated statistic vectors
    sample_mean_vec <- c(sample_mean_vec, mean(sim_sample, na.rm = TRUE))
    sample_median_vec <- c(sample_median_vec, median(sim_sample, na.rm = TRUE))
    sample_sd_vec <- c(sample_sd_vec, sd(sim_sample, na.rm = TRUE))
    sample_min_vec <- c(sample_min_vec, min(sim_sample, na.rm = TRUE))
    sample_max_vec <- c(sample_max_vec, max(sim_sample, na.rm = TRUE))
}
```

```{r}
simulation_stats <- tibble(
    median = sample_median_vec,
    sd = sample_sd_vec,
    min = sample_min_vec,
    max = sample_max_vec
)
```

```{r}
data.frame(
    average = sapply(simulation_stats, FUN = mean),
    standard_error = sapply(simulation_stats, FUN = sd)
) %>%
    kbl()
```

```{r, out.width="50%", fig.show='hold', echo=FALSE}
ggplot(simulation_stats, aes(x = median)) +
    geom_histogram(bins = 30) +
    labs(title = "Distribution of Medians")

ggplot(simulation_stats, aes(x = sd)) +
    geom_histogram(bins = 30) +
    labs(title = "Distribution of Standard Deviations")

ggplot(simulation_stats, aes(x = min)) +
    geom_histogram(bins = 30) +
    labs(title = "Distribution of Minimums")

ggplot(simulation_stats, aes(x = max)) +
    geom_histogram(bins = 30) +
    labs(title = "Distribution of Maximums")
```

```{r}
ggplot() +
    aes(x = sim_vec) +
    geom_histogram(bins = 50, color = "black") +
    labs(title = "Distribution of Simulated Values")
```

The distribution of the simulated values seems very much normal. The center is right around 50 and the count dips as the values get further and further away from the mean. 

***
\pagebreak
## 9.4: The Bootstrap
### Exercise 3

```{r}

B <- 1000
sample_df <- tibble(
    sample_medians = rep(NA, B),
    sample_sd = rep(NA, B),
    sample_min = rep(NA, B),
    sample_max = rep(NA, B),
)

sim_vec <- c()

for(i in 1:B) {
    resamp <- slice_sample(.data = iris,
                           n = 150,
                           replace = TRUE)
    
    # Simulated values
    sim_vec <- c(sim_vec, resamp$Petal.Width)
    
    # Simulated statistics
    sample_df$sample_medians[i] <- median(resamp$Petal.Width)
    sample_df$sample_sd[i] <- sd(resamp$Petal.Width)
    sample_df$sample_min[i] <- min(resamp$Petal.Width)
    sample_df$sample_max[i] <- max(resamp$Petal.Width)
}
```

```{r}
data.frame(
    average = sapply(sample_df, FUN = mean),
    standard_error = sapply(sample_df, FUN = sd)
) %>%
    kbl()
```

```{r, out.width="50%", fig.show='hold', echo=FALSE}
ggplot(sample_df, aes(x = sample_medians)) +
    geom_histogram(binwidth = .05) +
    labs(title = "Distribution of Medians")

ggplot(sample_df, aes(x = sample_sd)) +
    geom_histogram(bins = 30) +
    labs(title = "Distribution of Standard Deviations")

ggplot(sample_df, aes(x = sample_min)) +
    geom_histogram(bins = 30) +
    labs(title = "Distribution of Minimums")

ggplot(sample_df, aes(x = sample_max)) +
    geom_histogram(bins = 30) +
    labs(title = "Distribution of Maximums")

ggplot() +
    aes(x = sim_vec) +
    geom_histogram(bins = 50, color = "black") +
    labs(title = "Distribution of Simulated Values")
```


All of the plots here are very much **not** normal. The medians have two values that are far more frequent than any others, whereas the minimums and maximums only really have 1 value that occurs at all. The standard deviations seem to have a relatively normal distribution though, with a slight left skew. As for the general simulated values, There's a very non normal distribution with a peak close to 0 and a fair frequency of values between 1.3 and 1.8. 

***
\pagebreak
## 9.5: Outliers
### Exercise 4

```{r}
SnakeID <- 1:10
Ln <- c(85.7, 64.5, 84.1, 82.5, 78.0, 65.9, 81.3, 71.0, 86.7, 78.7)
Wt <- c(331.9, 121.5, 382.2, 287.3, 224.3, 380.4, 245.2, 208.2, 393.4, 228.3)
Snakes <- data.frame(SnakeID, Ln, Wt)
```

```{r, out.width="50%", fig.show='hold'}
ggplot(data = Snakes) +
    geom_histogram(mapping = aes(x = Ln),
                   fill = "blue",
                   color = "white",
                   bins = 5) +
    ggtitle("Histogram of Snakes Lengths")

ggplot(data = Snakes) +
    geom_histogram(mapping = aes(x = Wt),
                   fill = "blue",
                   color = "white",
                   bins = 5) +
    ggtitle("Histogram of Snakes Weights")
```

a) 

From these two histograms I absolutely cannot tell what the outlier is. There's too few bins to properly differentiate here.

b)

```{r}
ggplot(data = Snakes) +
    geom_point(mapping = aes(x = Ln, y = Wt)) +
    ggtitle("Scatterplot of Weights vs Lengths")
```

Here it is a lot easier to notice the outlier, it's the value in the top left.

***
\pagebreak
## 9.6: Statistical Models: Explaining Variation
### Exercise 5

```{r}
SnakeID <- 1:9
Ln <- c(85.7, 64.5, 84.1, 82.5, 78.0, 81.3, 71.0, 86.7, 78.7)
Wt <- c(331.9, 121.5, 382.2, 287.3, 224.3, 245.2, 208.2, 393.4, 228.3)
Snakes <- data.frame(SnakeID, Ln, Wt)
```

```{r}
ggplot(data = Snakes, mapping = aes(x = Ln, y = Wt)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
ggtitle("Scatterplot of Weights vs Lengths")
```

```{r}
my.reg <- lm(Wt ~ Ln, data = Snakes)
summary(my.reg)
```

From the output of summary(), the equation of the fitted line is:

\[\hat{Y} = -601.08 + 10.99x\]

Obtain the predicted weight for a snake whose length is 80cm in two ways:

1: By plugging 80 into the equation for x.

```{r}
fitted_line <- function(x) {
    -601.08 + (10.99*x)
}

fitted_line(80)
```
2: By using predict

```{r}
predict(my.reg, newdata = data.frame(Ln = 80))
```

b) Each additional cm of elongation typically adds around 10.99 units of weight to the snake. 5cm will add 55 units of weight. 

***
\pagebreak
### Exercise 6

```{r}
SF <- dplyr::filter(
    .data = nycflights13::flights,
    dest == "SFO",
    !is.na(arr_delay)
)
```

```{r, out.width="50%"}
ggplot(data = SF, mapping = aes(x = hour, y = dep_delay)) +
    geom_point() +
    geom_smooth(method = "lm") +
    xlab("Scheduled Hour of Departure") + ylab("Departure Delay (Minutes)") +
    coord_cartesian(ylim = c(-30, 200))
```

a) Use lm() and summary() to obtain the equation of the fitted regression line, with dep_delay as the response and hour as the explanatory variable.

```{r}
flight_reg <- lm(dep_delay ~ hour, data = SF)
summary(flight_reg)
```

The equation of the line is:
\[\hat{Y} = -10.956 + 1.86X\]

b) Obtained the predicted departure delay for a flight whose departure hour is 15 in two ways. Plug in 15 into the equation and use the predict command.

```{r}
-10.956 + (1.86 * 15)
```

```{r}
predict(flight_reg, newdata = data.frame(hour = 15))
```

c) The departure delay increases by around 1.86 minutes for every hour that passes. 

***
\pagebreak
### Exercise 7

```{r}
SnakeID <- 1:9
Ln <- c(85.7, 64.5, 84.1, 82.5, 78.0, 81.3, 71.0, 86.7, 78.7)
Wt <- c(331.9, 121.5, 382.2, 287.3, 224.3, 245.2, 208.2, 393.4, 228.3)
Snakes <- data.frame(SnakeID, Ln, Wt)
```

```{r}
snake_reg <- lm(Wt ~ Ln, data = Snakes)
```

a) What class of object is returned by lm()

```{r}
class(snake_reg)
```

```{r}
#B

is.list(snake_reg)
```

```{r}
#C

names(snake_reg)
```

```{r}
# D

Snakes <- dplyr::mutate(
    Snakes, FittedVals = snake_reg$fitted.values
)

ggplot(
    Snakes, aes(x = Ln, y = FittedVals)
) +
    geom_point() +
    ggtitle("Scatterplot of Fitted Values vs Lengths")
```

The plot of fitted values looks pretty much like a straight line, this makes sense considering we got these values from a linear regression.

```{r}
# E

Snakes <- dplyr::mutate(
    Snakes, Residuals = snake_reg$residuals
)

Snakes %>%
    ggplot(aes(x = Ln, y = Residuals)) +
    geom_point() +
    geom_hline(yintercept = 0) +
    ggtitle("Scatterplot of Residuals vs Lengths")
```

What we see is a scattering of points above or below the x-axis. This scattering of points just shows how far away an actual data point was from the predicted value.

```{r}
# F

sum(Snakes$Residuals)
```

We get a value here that is absurdly close to 0, it likely just isn't 0 due to rounding error. 

***
### Exercise 8

a) According to the output of summary the residual standard error appears to be 39.34 with 13171 degrees of freedom.

b) The value of $R^2$ seems to be 0.0427.

c) According to the provided table this linear model would be a very poor fit for the data. 

***
\pagebreak
### Exercise 9

```{r}
Sales <- c(174.4, 164.4, 244.2, 154.6, 181.6, 207.5, 152.8, 163.2,
           145.4, 137.2, 241.9, 191.1, 232.0, 145.3, 161.1, 209.7,
           146.4, 144.0, 232.6, 224.1, 166.5)
Under16 <- c(68.5, 45.2, 91.3, 47.8, 46.9, 66.1, 49.5, 52.0, 48.9,
             38.4, 87.9, 72.8, 88.4, 42.9, 52.5, 85.7, 41.3, 51.7,
             89.6, 82.7, 52.3)
Income <- c(16.7, 16.8, 18.2, 16.3, 17.3, 18.2, 15.9, 17.2, 16.6, 16.0,
            18.3, 17.1, 17.4, 15.8, 17.8, 18.4, 16.5, 16.3, 18.1, 19.1,
            16.0)
portraitSales <- data.frame(Sales, Under16, Income)
```

```{r}
# A

sales_reg <- lm(Sales ~ Under16 + Income, data = portraitSales)

sales_reg %>% summary()
```

The equation we get for this set of data is:

\[\hat{Y} = -68.867 + 1.455x_1 + 9.366x_2\]

```{r}
# B1

-68.867 + (1.455 * 45) + (9.366 * 17)
```

```{r}
# B2

predict(sales_reg, newdata = data.frame(Under16 = 45.0, Income = 17))
```

c) Using the equation of the plane, we can surmise that sales increases by approximately **1.455** per 1.0 thousand people under 16.

d) Using the same equation, we can say sales increase by approximately **9.366**.

***
\pagebreak
### Exercise 10

```{r}
cdi <- read.table("CDI.txt", header = TRUE)
```


```{r}
# A

cdi %>%
    dplyr::select(-c(ID, County, State)) %>%
    pairs()
```

```{r, fig}
# B

cdi %>%
    dplyr::select(-c(ID, County, State)) %>%
    cor() %>%
    kableExtra::kbl() %>%
    kableExtra::kable_classic() %>%
    kable_styling(latex_options=c("scale_down", "hold_position"))
```

```{r}
# C

act_phys_reg <- lm(nActPhys ~ TotPop + LandArea + TotInc, data = cdi)

act_phys_reg %>% summary()
```
The equation of this line is:

\[Y = (-1.332 * 10) + (8.336 \cdot 10^{-4}X_1) + (-6.552 \cdot 10^{-2}X_2) + (9.413 \cdot 10^{-2})\]

```{r}
# D1
a <- 8.336 * 10^(-4) * 400000
b <- -6.552 * 10^(-2) * 1000
c <- 9.413 * 10^(-2) * 8000

-1.332 * 10 + a + b + c

```

```{r}
# D2

predict(
    act_phys_reg, 
    newdata = data.frame(
        TotPop = 400000,
        LandArea = 1000,
        TotInc = 8000
    )
)
```
e)

The number of active physicians increases by $8.336 \cdot 10^{-4}$ for every additional personal added to the population.

f)

The number of active physicians increases by $9.413 \cdot 10^{-2}$ for every additional 1.0 million dollars in total personal income. 

***
\pagebreak
### Exercise 11

```{r}
cdi <- cdi %>%
    dplyr::mutate(PopDens = TotPop / LandArea)
```

```{r}
act_phys_dens_reg <- lm(nActPhys ~ PopDens + PctPop65 + PerCapInc, data = cdi)
```

```{r}
act_phys_dens_reg %>% summary()
```

The equation of this plane is:
\[Y = -1087.8142 + 0.2873X_1 - 7.9640X_2 + 0.1033X_3\]

```{r}
# B1

- 1087.8142 + (0.2873*900) - (7.9640*15) + (0.1033*20000)
```

```{r}
# B2

predict(
    act_phys_dens_reg, 
    newdata = data.frame(
        PopDens = 900,
        PctPop65 = 15,
        PerCapInc = 20000
    )
)
```

***
\pagebreak
### Exercise 12

a) The residual standard error of the model using total population has a residual standard error of **560.4 on 436 degrees of freedom**. The residual standard error of the other model has an RSE of **1589 on 436 degrees of freedom**. This shows that the first model we used was probably the better model to use.

b) 

Total Population Model:
\[R^2 = .9026\]

Population Density Model:
\[R^2 = .2173\]

This mirrors our results from part a, this also shows the total population model as being a better predictor for the number of active physicians.

c) Total population is a better predictor for the number of active physicians. Population density does not seem to be a good predictor of this information at all.

***
\pagebreak
## 9.7: Logistic Regression: Dichotomous (0 or 1) Response Variable.
### Exercise 13

```{r}
dues <- read.table("DUES.txt", header = TRUE)
```

```{r}
my.logreg <- glm(NotRenew ~ DuesIncr, family = "binomial", data = dues)
```

```{r}
# A1

num <- exp(1)^(-15.42 + (0.39*45))
den <- 1 + exp(1)^(-15.42 + (0.39*45))

num / den
```

```{r}
# A2
newDues <- data.frame(DuesIncr = 45)
predict(my.logreg, newDues, type = "response")
```

```{r}
# B1 

num <- exp(1)^(-15.42 + (0.39*35))
den <- 1 + exp(1)^(-15.42 + (0.39*35))

num / den
```

```{r}
# B2

newDues <- data.frame(DuesIncr = 35)
predict(my.logreg, newDues, type = "response")
```

