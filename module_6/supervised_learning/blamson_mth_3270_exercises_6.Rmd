---
title: "Exercises 6"
author: "Brady Lamson"
date: "`r Sys.Date()`"
output: 
    pdf_document:
            includes:
                in_header: "preamble.tex"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(rpart)
library(ggplot2)
library(randomForest)
```

# 11: Supervised Learning
## 11.1 Classifiers
### Exercise 1

Based on the decision tree I would make the following predictions:

- A 9-inch, 10-pound pet would likely be a **cat** and be pretty confident. 

- A 14-inch, 21-pound pet would likely be a **dog** and be very confident. 

***
### Exercise 2

```{r}
type <- c("dog", "dog", "cat", "dog", "cat", "dog", "cat", "dog", "cat",
          "dog", "cat", "dog", "dog", "cat", "dog", "cat", "dog", "cat", "dog")

wt <- c(8, 17, 8, 18, 7, 22, 6, 16, 7, 20, 10, 15, 14, 11, 13, 13, 15, 17, 10)
ht <- c(7.5, 10, 8, 15, 7, 15, 7, 13, 11, 16, 7, 10.5, 9, 9.5, 9, 8, 9, 8, 12)

pets <- data.frame(Type = type, Ht = ht, Wt = wt)

my.tree <- rpart(
    Type ~ Wt + Ht, data = pets,
    control = rpart.control(minsplit = 7)
)

newPets <- data.frame(Ht = c(9, 14), Wt = c(10, 21))

predict(my.tree, newdata = newPets, type = "class")
```

First pet is predicted to be a cat and the second pet is predicted to be a dog. This prediction makes the same predictions I made in exercise 1. 

***
\pagebreak
### Exercise 3

```{r}
my.tree <- rpart(Species ~ Petal.Length + Petal.Width, data = iris)
my.tree
```

a) This tree has 3 different terminal nodes.

```{r}
predSpecies <- predict(my.tree, type = "class")
species <- data.frame(Actual = iris$Species, Predicted = predSpecies)

confusion <- table(species)
confusion
```

b) The trees correct classification rate is **96%** and the mis-classification rate is, by extension, **4%**.

```{r}
## First plot:
par(xpd = TRUE)
plot(my.tree, compress = TRUE)
text(my.tree, use.n = TRUE)
par(xpd = FALSE)
```

```{r}
## Second plot:
splitPetal.Length <- 2.45
splitPetal.Width <- 1.75
splitLines <- data.frame(x1 = splitPetal.Length, x2 = 7,
                         y1 = splitPetal.Width, y2 = splitPetal.Width)

g <- ggplot(data = iris,
            mapping = aes(x = Petal.Length, y = Petal.Width,
                          color = Species)) +
    geom_point() +
    labs(title = "Widths and Lengths of Petals") +
    geom_vline(xintercept = splitPetal.Length,
               color = "dodgerblue",
               linetype = 2) +
    geom_segment(data = splitLines,
                 mapping = aes(x = x1, y = y1, xend = x2, yend = y2),
                 color = "rosybrown", linetype = 2)
g
```

c) Using the above plots I make the following predictions.

- A flower with `Petal.Length` of 3.0cm and `Petal.Width` of 1.5cm would be **veriscolor**.

A flower with `Petal.Length` of 4.0cm and `Petal.Width` of 2.1cm would be **virginica**. There aren't any virginica flowers with that length, but the cutoffs on the plot show that's the best prediction to make. 

```{r}
newIris <- 
    data.frame(Petal.Length = c(3.0, 4.0),
               Petal.Width = c(1.5, 2.1)
    )
```

```{r}
predict(my.tree, newdata = newIris, type = "class")
```

The predictions made by the function match that of the predictions I made in part c.

***
\pagebreak
### Exercise 4

```{r}
my.tree <- rpart(Species ~ Sepal.Length + Sepal.Width, data = iris)
my.tree
```

a) This tree has **5** terminal nodes

```{r}
predSpecies <- predict(my.tree, type = "class")
species <- data.frame(Actual = iris$Species, Predicted = predSpecies)
confusion <- table(species)
confusion
```

```{r}
# B

classification_rate <- sum(diag(confusion) / nrow(species))

glue::glue("
    The correct classification rate is approximately {(classification_rate * 100) %>% round(3)}%,
    The misclassification rate is approximately {((1 - classification_rate) * 100) %>% round(3)}%
")
```

```{r}
## First plot:
par(xpd = TRUE)
plot(my.tree, compress = TRUE)
text(my.tree, use.n = TRUE)
par(xpd = FALSE)
```

```{r}
## Second plot:
split1Sepal.Length <- 5.45
split1Sepal.Width <- 2.8
splitLines1 <- data.frame(
    x1 = 3, x2 = split1Sepal.Length,
    y1 = split1Sepal.Width, y2 = split1Sepal.Width
)

split2Sepal.Length <- 6.15
split2Sepal.Width <- 3.1
splitLines2 <- data.frame(
    x1 = split1Sepal.Length, x2 = split2Sepal.Length,
    y1 = split2Sepal.Width, y2 = split2Sepal.Width
)

g <- ggplot(data = iris,
            mapping = aes(x = Sepal.Length, y = Sepal.Width,
                          color = Species)) +
    geom_point() +
    labs(title = "Widths and Lengths of Sepals") +
    geom_vline(xintercept = split1Sepal.Length,
               color = "dodgerblue",
               linetype = 2) +
    geom_vline(xintercept = split2Sepal.Length,
               color = "purple",
               linetype = 2) +
    geom_segment(data = splitLines1,
                 mapping = aes(x = x1, y = y1, xend = x2, yend = y2),
                 color = "brown", linetype = 2) +
    geom_segment(data = splitLines2,
                 mapping = aes(x = x1, y = y1, xend = x2, yend = y2),
                 color = "red", linetype = 2)
g
```

c) Using the above plots I make the following predictions.

- A flower with `Petal.Length` of 6.0cm and `Petal.Width` of 3.5cm would be **setosa**. I used the decision tree over the scatterplot for this decision.

A flower with `Petal.Length` of 7.0cm and `Petal.Width` of 3.0cm would be **virginica**. 

```{r}
newIris <- data.frame(Sepal.Length = c(6.0, 7.0),
Sepal.Width = c(3.5, 3.0))

predict(my.tree, newdata = newIris, type = "class")
```

The same decisions as part c were made, the prediction simply appears to match the decision tree cases.

***
\pagebreak
### Exercise 5

```{r}
y1 <- c("A", "B", "C", "C", "C", "C", "C", "C", "C", "C", "C", "C")
round(prop.table(table(y1)), digits = 1)

y2 <- c("A", "B", "C", "C", "A", "C", "B", "A", "A", "B", "C", "B")
round(prop.table(table(y2)), digits = 1)

```

a) 

- Based just on the fact that more homogeneous data scores higher on the Gini index, I would guess `y1` would be considered more *pure* and thus have a lower score. There are far more C's than anything else. `y2` is too evenly split to be considered homogeneous and as such would be further from 0 than `y1`. 

```{r}
1 - sum(
    c(
        .1^2, .1^2, .8^2
    )
)
```

.34 is the gini index for `y1` assuming I did the calculation correctly.

```{r}
1 - sum(
    c(
        .3^2, .3^2, .3^2
    )
)
```

.73 is the gini index for `y2` assuming I did the calculation correctly. This would, at the very least, match my earlier assumptions that this would be higher than the gini index for `y1`. 

b) The gini index for a vector with all of the same values in them is 0, because the vector is completely pure. $1 - 1^2 = 0$.

***
\pagebreak
### Exercise 6

```{r}
my.tree <- rpart(Species ~ Sepal.Length + Sepal.Width, data = iris,
                 control = rpart.control(cp = 0.002))
my.tree

par(xpd = TRUE)
plot(my.tree, compress = TRUE)
text(my.tree, use.n = TRUE)
par(xpd = FALSE)
```

```{r}
my.tree <- rpart(Species ~ Sepal.Length + Sepal.Width, data = iris,
                 control = rpart.control(cp = 0.05))
my.tree

par(xpd = TRUE)
plot(my.tree, compress = TRUE)
text(my.tree, use.n = TRUE)
par(xpd = FALSE)
```

**0.2%** Resulted in far more terminal nodes.

\pagebreak
### Exercise 7

```{r}
my.forest <- randomForest(Species ~ Sepal.Length + Sepal.Width +
                              Petal.Length + Petal.Width,
                          data = iris,
                          ntree = 500,
                          mtry = 1)

sum(diag(my.forest$confusion)) / nrow(iris)
```

a) An `mtry` of 1 gives a correct classification rate of approximately 94.67%.

```{r}
my.forest <- randomForest(Species ~ Sepal.Length + Sepal.Width +
                              Petal.Length + Petal.Width,
                          data = iris,
                          ntree = 500,
                          mtry = 3)

sum(diag(my.forest$confusion)) / nrow(iris)
```

b) An `mtry` of 3 gives a correct classification rate of 96%.

***
\pagebreak
### Exercise 8

```{r}
my.forest <- randomForest(Species ~ Sepal.Length + Sepal.Width +
                              Petal.Length + Petal.Width,
                          data = iris,
                          ntree = 500,
                          mtry = 4)

importance(my.forest)
```

a) `Petal.Width` is the most important, `Sepal.Length` is the least important.

```{r}
newIris <-
    data.frame(Petal.Length = c(3.0, 2.2, 2.7),
               Petal.Width = c(1.2, 2.1, 1.6),
               Sepal.Length = c(5.5, 5.1, 5.9),
               Sepal.Width = c(3.0, 2.7, 3.2))

predict(my.forest, newdata = newIris, type = "class")
```

b) The three predictions were **veriscolor**, **setosa**, **veriscolor**. 
 
***
\pagebreak
### Exercise 9

```{r}
# A

train_iris <- select(iris, -c(Species, Sepal.Length, Sepal.Width))
ret_vec <- c()

for (ktry in 3:8) {
    
    # k nearest neighbor classification procedure:
    my.knn <- class::knn(train = train_iris,
                         test = train_iris,
                         cl = iris$Species,
                         k = ktry)
    
    # Save actual and predicted species:
    species <- data.frame(Actual = iris$Species, Predicted = my.knn)
    
    # Obtain correct classification rate:
    confusion <- table(species)
    
    ret_vec <- c(ret_vec, sum(diag(confusion)) / nrow(iris))
}

ret_vec
```

```{r}
# B

# Data frame containing new flower for classification:
newIris <- data.frame(Petal.Length = 4.35,
                      Petal.Width = 1.65)
predict_vec <- c()

for (ktry in seq(from = 3, to = 21, by = 3)) {
    # k nearest neighbor classification procedure:
    my.knn <- class::knn(train = train_iris,
                         test = newIris,
                         cl = iris$Species,
                         k = ktry)
    
    predict_vec <- c(predict_vec, my.knn)
}
predict_vec
```

The prediction does not change for any of the provided values of `ktry` I used.

***
\pagebreak
### Exercise 10

```{r}
rock2 <- 
    rock %>%
    dplyr::mutate(
        area = area / 10000,
        peri = peri / 10000,
        perm = log(perm)
    )

ret_vec <- c()

for (ktry in c(1,3,5,7)) {
    my.nn <- 
        nnet::nnet(
            perm ~ area + peri + shape, data = rock2,
            size = ktry, linout = TRUE, maxit = 1000, trace = FALSE
        )
    
    rss <- sum((rock2$perm - predict(my.nn))^2)
    
    ret_vec <- c(ret_vec, rss)
}

ret_vec
```

**7** resulted in the smallest residual sum of squares.

***
\pagebreak
### Exercise 11

```{r}
# Neural network classification procedure with k = 2 hidden units:
my.nn <- 
    nnet::nnet(
        Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,
        data = iris, size = 2, maxit = 200, trace = FALSE
    )

newIris <- 
    data.frame(
        Petal.Length = c(3.5, 4.7, 1.3),
        Petal.Width = c(1.0, 1.5, 0.5),
        Sepal.Length = c(5.2, 6.2, 4.8),
        Sepal.Width = c(3.3, 3.6, 2.3)
    )

predict(my.nn, newdata = newIris, type = "class")
print("--------------------------------------")
predict(my.nn, newdata = newIris)
```

Based on the given probabilities, we can see that **veriscolor** has the largest probabilities for the first 2 new flowers. **Setosa** is the highest for the final new flower. 

***
\pagebreak
### Exercise 12

```{r}
Ln <- c(85.7, 64.5, 84.1, 82.5, 78.0, 81.3, 71.0, 86.7, 78.7)
Wt <- c(331.9, 121.5, 382.2, 287.3, 224.3, 245.2, 208.2, 393.4, 228.3)
snakes <- data.frame(Length = Ln, Weight = Wt)

newSnakes <- 
    data.frame(
        Length = c(67, 72, 77, 81, 86),
        Weight = c(127.9, 153.7, 204.7, 300.6, 291.4)
    )

mod0 <- lm(Weight ~ 1, data = snakes)
mod1 <- lm(Weight ~ Length, data = snakes)
mod2 <- lm(Weight ~ poly(Length, 2, raw = TRUE), data = snakes)
mod3 <- lm(Weight ~ poly(Length, 3, raw = TRUE), data = snakes)
mod4 <- lm(Weight ~ poly(Length, 4, raw = TRUE), data = snakes)
mod5 <- lm(Weight ~ poly(Length, 5, raw = TRUE), data = snakes)
```

```{r}
# A

rmse0 <- sqrt(
    mean(
        (newSnakes$Weight - predict(mod0, newdata = newSnakes))^2
    )
)

rmse1 <- sqrt(
    mean(
        (newSnakes$Weight - predict(mod1, newdata = newSnakes))^2
    )
)

rmse2 <- sqrt(
    mean(
        (newSnakes$Weight - predict(mod2, newdata = newSnakes))^2
    )
)

rmse3 <- sqrt(
    mean(
        (newSnakes$Weight - predict(mod3, newdata = newSnakes))^2
    )
)

rmse4 <- sqrt(
    mean(
        (newSnakes$Weight - predict(mod4, newdata = newSnakes))^2
    )
)

rmse5 <- sqrt(
    mean(
        (newSnakes$Weight - predict(mod5, newdata = newSnakes))^2
    )
)

rmse0
rmse1
rmse2
rmse3
rmse4
rmse5
```

a) The best model according to RMSE is the 2nd. It has the lowest value.

```{r}
mae0 <- mean(abs(newSnakes$Weight - predict(mod0, newdata = newSnakes)))
mae1 <- mean(abs(newSnakes$Weight - predict(mod1, newdata = newSnakes)))
mae2 <- mean(abs(newSnakes$Weight - predict(mod2, newdata = newSnakes)))
mae3 <- mean(abs(newSnakes$Weight - predict(mod3, newdata = newSnakes)))
mae4 <- mean(abs(newSnakes$Weight - predict(mod4, newdata = newSnakes)))
mae5 <- mean(abs(newSnakes$Weight - predict(mod5, newdata = newSnakes)))

mae0
mae1
mae2
mae3
mae4
mae5
```

The best model according to MAE is the 3rd model with the 2nd trailing close behind. 